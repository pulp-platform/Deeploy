

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />
    <link href="../_static/DeeployIconGreen.svg" rel="icon" type="image/svg+xml">
    <link href="../_static/DeeployIconGreen-32x32.png" sizes="32x32" rel="icon" type="image/png">
    <link href="../_static/DeeployIconGreen-64x64.png" sizes="64x64" rel="icon" type="image/png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Neural Network Deeployment on the PULP Platform &mdash; Deeploy 2024 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=9edc463e" />

  
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=6fefd858"></script>
      <script src="../_static/doctools.js?v=fd6eb6e6"></script>
      <script src="../_static/sphinx_highlight.js?v=6ffebe34"></script>
      <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@4/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Debugging" href="debugging.html" />
    <link rel="prev" title="Tutorials" href="overview.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: white" >

          
          
          <a href="../index.html">
            
              <img src="../_static/DeeployBannerGreen-640x-320.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../install.html">Quickstart</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="overview.html">Tutorials</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">Neural Network Deeployment on the PULP Platform</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#installation">Installation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#deeploy-101">Deeploy 101</a></li>
<li class="toctree-l3"><a class="reference internal" href="#micro-llama-on-siracusa">Micro Llama on Siracusa</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#transformers-101">Transformers 101</a></li>
<li class="toctree-l4"><a class="reference internal" href="#the-siracusa-platform">The Siracusa Platform</a></li>
<li class="toctree-l4"><a class="reference internal" href="#tiling-basics">Tiling Basics</a></li>
<li class="toctree-l4"><a class="reference internal" href="#profiling-the-execution">Profiling the Execution</a></li>
<li class="toctree-l4"><a class="reference internal" href="#using-the-npu-and-the-neural-memory-subsystem-nms">Using the NPU and the Neural Memory Subsystem (NMS)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="debugging.html">Debugging</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../structure.html">Library Structure</a></li>
<li class="toctree-l1"><a class="reference internal" href="../apidocs.html">API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../releasing.html">Deeploy Release Guide</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: white" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Deeploy</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="overview.html">Tutorials</a></li>
      <li class="breadcrumb-item active">Neural Network Deeployment on the PULP Platform</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/tutorials/introduction.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div style="display: flex; justify-content: space-between; align-items: center;">
 <img src="../_static/tutorials/introduction/EthLogoPos.png" alt="Image" width="300">
 <div style="text-align: right;">
 <p>Institut für Integrierte Systeme <br>
 Integrated Systems Laboratory</p>
 </div>
</div>
<section class="tex2jax_ignore mathjax_ignore" id="neural-network-deeployment-on-the-pulp-platform">
<h1>Neural Network Deeployment on the PULP Platform<a class="headerlink" href="#neural-network-deeployment-on-the-pulp-platform" title="Link to this heading"></a></h1>
<p>Author: <em>Victor J.B Jung</em> <br>
Date: 27th May 2025</p>
<section id="installation">
<h2>Installation<a class="headerlink" href="#installation" title="Link to this heading"></a></h2>
<p><strong>⚠️ DISCLAIMER: The current container and commit are from main and devel, they will be tagged in the next release</strong></p>
<p>Clone Deeploy and its submodules:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">git</span> <span class="n">clone</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">github</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">pulp</span><span class="o">-</span><span class="n">platform</span><span class="o">/</span><span class="n">Deeploy</span><span class="o">.</span><span class="n">git</span> <span class="o">&amp;&amp;</span> <span class="n">cd</span> <span class="n">Deeploy</span>
<span class="n">git</span> <span class="n">submodule</span> <span class="n">update</span> <span class="o">--</span><span class="n">init</span> <span class="o">--</span><span class="n">recursive</span>
</pre></div>
</div>
<p>Pull the docker image:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">docker</span> <span class="n">pull</span> <span class="n">ghcr</span><span class="o">.</span><span class="n">io</span><span class="o">/</span><span class="n">pulp</span><span class="o">-</span><span class="n">platform</span><span class="o">/</span><span class="n">deeploy</span><span class="p">:</span><span class="n">main</span>
</pre></div>
</div>
<p>Run the container and bind Deeploy’s folder in the container:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>docker run -it --name deeploy_main -v $(pwd):/app/Deeploy ghcr.io/pulp-platform/deeploy:main
</pre></div>
</div>
<p>Install Deeploy inside the container:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cd</span> <span class="n">Deeploy</span>
<span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">e</span> <span class="o">.</span> <span class="o">--</span><span class="n">extra</span><span class="o">-</span><span class="n">index</span><span class="o">-</span><span class="n">url</span><span class="o">=</span><span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">pypi</span><span class="o">.</span><span class="n">ngc</span><span class="o">.</span><span class="n">nvidia</span><span class="o">.</span><span class="n">com</span>
</pre></div>
</div>
<p>From the <code class="docutils literal notranslate"><span class="pre">DeeployTest</span></code> folder, you can use the <code class="docutils literal notranslate"><span class="pre">testRunner</span></code> to compile ONNXs and execute the output code using the appropriate simulators.</p>
<p>To validate your installation, you can run a simple Add node on each platform:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">testRunner_generic</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">t</span> <span class="n">Tests</span><span class="o">/</span><span class="n">IntKernels</span><span class="o">/</span><span class="n">Add</span><span class="o">/</span><span class="n">Regular</span>
<span class="n">python</span> <span class="n">testRunner_cortexm</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">t</span> <span class="n">Tests</span><span class="o">/</span><span class="n">IntKernels</span><span class="o">/</span><span class="n">Add</span><span class="o">/</span><span class="n">Regular</span>
<span class="n">python</span> <span class="n">testRunner_mempool</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">t</span> <span class="n">Tests</span><span class="o">/</span><span class="n">IntKernels</span><span class="o">/</span><span class="n">Add</span><span class="o">/</span><span class="n">Regular</span>
<span class="n">python</span> <span class="n">testRunner_snitch</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">t</span> <span class="n">Tests</span><span class="o">/</span><span class="n">IntKernels</span><span class="o">/</span><span class="n">Add</span><span class="o">/</span><span class="n">Regular</span><span class="o">/</span>
<span class="n">python</span> <span class="n">testRunner_siracusa</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">t</span> <span class="n">Tests</span><span class="o">/</span><span class="n">IntKernels</span><span class="o">/</span><span class="n">Add</span><span class="o">/</span><span class="n">Regular</span> <span class="o">--</span><span class="n">cores</span><span class="o">=</span><span class="mi">8</span>
</pre></div>
</div>
<p>Once all these basic tests are passed, we can jump into the basics of Deeploy.</p>
</section>
<section id="deeploy-101">
<h2>Deeploy 101<a class="headerlink" href="#deeploy-101" title="Link to this heading"></a></h2>
<p>Deeploy is a compiler that transforms static computational graph (represented with the <a class="reference external" href="https://onnx.ai/onnx/operators/">ONNX format</a>) into bare-metal and (hopefully) optimized <a class="reference external" href="https://www.c-language.org/">C</a>. More specifically, it generates an application that can be deployed on the desired platform.</p>
<p>Hence, Deeploy’s inputs are:</p>
<ul class="simple">
<li><p>An ONNX file describing your neural network.</p></li>
<li><p>Input tensors.</p></li>
<li><p>Expected output tensors generated with your favorite framework (ONNXRuntime or Torch, for instance).</p></li>
</ul>
<p>Deeploy is shipped with a comprehensive testing framework conveniently named DeeployTest. This testing framework contains Test Runners for end-to-end testing of your network on a given platform. More specifically, a Test Runner compiles a given ONNX file, builds the project, feeds the inputs into the compiled neural network, and compares the output with the golden values to ensure correctness.</p>
<p>If you followed this tutorial correctly, you already used Test Runners (e.g., <code class="docutils literal notranslate"><span class="pre">testRunner_siracusa.py</span></code>) to validate the Deeploy installation! We will dive into the details of the Test Runners CLI very soon, but first, let’s look at the tools and libraries used downstream in Deeploy.</p>
<p>The figure below gives an overview of the deployment stack. As you can see, there are several steps to take before actually running the application. For the build system (<em>e.g.,</em> the tool to organize compilation and linking), we use <a class="reference external" href="https://cmake.org/">CMake</a>. The default C compiler shipped with Deeploy is <a class="reference external" href="https://llvm.org/">LLVM 15</a>, but it supports GCC, given that you provide a local installation. To generate the Application Binary, we link the Network Code with the necessary Kernel Libraries and a Standard C Library (here <a class="reference external" href="https://github.com/picolibc/picolibc">Picolibc</a>). Then, we feed this Application Binary to the appropriate simulator; from there, you can verify the correctness and benchmark the application.</p>
<p align="center">
 <img src="../_static/tutorials/introduction/DeeploySystem.png" alt="Description" width="60%">
</p>
<p>You can visualize the ONNX graphs using <a class="reference external" href="https://netron.app/">Netron</a>. Either use the web interface or install the python package with <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">netron</span></code>.</p>
<blockquote>
<div><p>✅ <strong>Task:</strong> Visualize the ONNX graph of the <code class="docutils literal notranslate"><span class="pre">IntKernels/Add/Regular</span></code>, <code class="docutils literal notranslate"><span class="pre">Models/MobileNetv2</span></code>, and <code class="docutils literal notranslate"><span class="pre">Others/Transformer</span></code></p>
</div></blockquote>
<p>The ONNX graphs are in <code class="docutils literal notranslate"><span class="pre">DeeployTest/Tests/&lt;TestName&gt;/network.onnx</span></code>. The networks are increasing in complexity, <code class="docutils literal notranslate"><span class="pre">IntKernels/Add/Regular</span></code> is a single node network for unit testing, while <code class="docutils literal notranslate"><span class="pre">Models/MobileNetv2</span></code> is a simple sequential network mostly made of convolutions. Finally, the <code class="docutils literal notranslate"><span class="pre">Others/Transformer</span></code> network showcases a typical transformer block used in Encoder and Decoder networks. If you want to peek at a complex network, you can visualize <code class="docutils literal notranslate"><span class="pre">Models/microLlama/microLlama128</span></code>.</p>
<p>Now that we understand Deeploy’s input, let’s check the output-generated code!</p>
<blockquote>
<div><p>✅ <strong>Task:</strong> Take a look at the code generated by Deeploy for the Generic platform.</p>
</div></blockquote>
<p>The generated code is located in the following directory: <code class="docutils literal notranslate"><span class="pre">DeeployTest/TEST_&lt;PlatformName&gt;/Tests</span></code>, and the <code class="docutils literal notranslate"><span class="pre">Network.c</span></code> file is the interesting one.</p>
<p>The generated code is trivial for the <code class="docutils literal notranslate"><span class="pre">IntKernels/Add/Regular</span></code> graph; we simply use the template for the <code class="docutils literal notranslate"><span class="pre">Add</span></code> node of the Generic platform. You can find the template declaration in <code class="docutils literal notranslate"><span class="pre">Deeploy/Targets/Generic/Templates/AddTemplate.py</span></code>.</p>
<p>Now, if you want to look at something a bit more complex, run <code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">testRunner_generic.py</span>&#160; <span class="pre">-t</span> <span class="pre">./Tests/Models/miniMobileNetv2</span></code> (from <code class="docutils literal notranslate"><span class="pre">DeeployTest</span></code>) and look at the generated code. There are two interesting points you can notice:</p>
<ul class="simple">
<li><p>We hoist the constants at the top of the file.</p></li>
<li><p>In the <code class="docutils literal notranslate"><span class="pre">RunNetwork</span></code> function, we sequentially have node templates to execute the operands and malloc/free to manage the memory. You can open the ONNX graph of <code class="docutils literal notranslate"><span class="pre">Models/miniMobileNetv2</span></code> on the side to try to match the nodes of the graph with their generated code.</p></li>
</ul>
<blockquote>
<div><p>✅ <strong>Task:</strong> Visualize the effect of passes on the ONNX graph for the Siracusa platform.</p>
</div></blockquote>
<p>Deeploy applies passes on the ONNX graph to transform its topology and optimize its execution. Let’s visualize the effect of the passes used in the Siracusa Platform. First, let’s execute our <code class="docutils literal notranslate"><span class="pre">miniMobileNetv2</span></code> on Siracusa with <code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">testRunner_siracusa.py</span>&#160; <span class="pre">-t</span> <span class="pre">./Tests/Models/miniMobileNetv2</span></code>. You can find the original ONNX graph at <code class="docutils literal notranslate"><span class="pre">DeeployTest/Tests/Models/miniMobileNetv2/network.onnx</span></code>, and the transformed ONNX graph at <code class="docutils literal notranslate"><span class="pre">DeeployTest/TEST_SIRACUSA/Tests/Models/miniMobileNetv2/deeployStates/backend_post_binding.onnx</span></code>. Open both ONNX graphs side by side to compare them.</p>
<p>You can notice the effect of two passes on the graph:</p>
<ul class="simple">
<li><p>One pass fuses the <code class="docutils literal notranslate"><span class="pre">Conv</span></code> and <code class="docutils literal notranslate"><span class="pre">RequantShift</span></code> nodes. This is a common technique named <a class="reference external" href="https://medium.com/data-science/how-pytorch-2-0-accelerates-deep-learning-with-operator-fusion-and-cpu-gpu-code-generation-35132a85bd26">Operator Fusion</a> and used in many DNN compilers.</p></li>
<li><p>Another pass is adding a <code class="docutils literal notranslate"><span class="pre">Transpose</span></code> node before the <code class="docutils literal notranslate"><span class="pre">RequantizedConv</span></code> in order to align the tensor layout from CHW to HWC (where C = Channels, H = Height, and W = Width). The HWC tensor layout is required to use optimized Convolution kernels (to learn more, check out <a class="reference external" href="https://www.intel.com/content/www/us/en/developer/articles/technical/pytorch-vision-models-with-channels-last-on-cpu.html">this blog post</a>).</p></li>
</ul>
<p>Now that you understand the basics of Deeploy let’s jump into the optimized deployment of a small language model on the Siracusa SoC.</p>
</section>
<section id="micro-llama-on-siracusa">
<h2>Micro Llama on Siracusa<a class="headerlink" href="#micro-llama-on-siracusa" title="Link to this heading"></a></h2>
<section id="transformers-101">
<h3>Transformers 101<a class="headerlink" href="#transformers-101" title="Link to this heading"></a></h3>
<p>In this section, we will study the optimization of the deployment of a small language model. To fully understand this section, you need some basic understanding of Transformer’s architecture and Language Model inference mode. If you need a refresher on Transformer’s architecture, check out the <em>Transformer Basics</em> section of <a class="reference external" href="https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/#transformer-basics">Lilian Weng’s blog post</a>.</p>
<p>Now, Language Models have two inference modes:</p>
<ul class="simple">
<li><p>The <strong>Parallel Mode</strong> (AKA <em>Prefill Mode</em>) is used to process the tokens of the prompts in parallel and generate the KV cache of the prompt and the first token of the Language Model’s “reply”. This mode contains mostly GEMMs.</p></li>
<li><p>The <strong>Autoregressive Mode</strong> generates the rest of the Language Model’s reply. It uses the KV cache from the previous step, generates a new KV cache entry, and predicts the next token. This mode contains mostly GEMVs.</p></li>
</ul>
<p>To summarize, to generate a Language Model reply of <span class="math notranslate nohighlight">\(N\)</span> tokens, there is:</p>
<ul class="simple">
<li><p>One <strong>Parallel Mode</strong> inference to process the prompt and generate the first token.</p></li>
<li><p><span class="math notranslate nohighlight">\(N-1\)</span> <strong>Autoregressive Mode</strong> inferences to generate the rest of the tokens.</p></li>
</ul>
<p>The slide below visually represents the <strong>Parallel Mode</strong> and <strong>Autoregressive Mode</strong>.</p>
<p align="center">
 <img src="../_static/tutorials/introduction/Victor_Jung_EDGEAIForumDeeploy_S5.png" alt="Description" width="75%">
</p>
</section>
<section id="the-siracusa-platform">
<h3>The Siracusa Platform<a class="headerlink" href="#the-siracusa-platform" title="Link to this heading"></a></h3>
<p>Let’s also quickly refresh our knowledge of the Siracusa platform to understand what kind of hardware we must deploy on. Below is the high-level block diagram of Siracusa, compute-wise we will mainly use:</p>
<ul class="simple">
<li><p>The cluster of RV32 cores, they are modified to be great at crunching numbers. They feature <a class="reference internal" href="#"><span class="xref myst">SIMD</span></a>, hardware loops (see the <a class="reference external" href="https://www.pulp-platform.org/docs/ri5cy_user_manual.pdf">RI5CY user manual</a>, p17), and the <a class="reference external" href="https://pulp-platform.org/docs/hipeac/acaces2021/04_PULP_Accelerators.pdf">XPULP</a> ISA extensions.</p></li>
<li><p>The <a class="reference external" href="https://github.com/pulp-platform/neureka">NEUREKA</a> NPU, an accelerator targeting integer convolutions.</p></li>
</ul>
<p>In terms of memories, we have:</p>
<ul class="simple">
<li><p>L3: An off-chip RAM (not shown on the block diagram) of 16MB capacity. The L3 has its own DMA that can transfer data to L2.</p></li>
<li><p>Neural Memory Subsystem (NMS): An SRAM/MRAM-based <em>Weight Memory</em> to store constants with a direct link to the NPU.</p></li>
<li><p>L2: An on-chip SRAM-based L2 memory of 2MB.</p></li>
<li><p>L1: A TCDM memory of size 256KB.</p></li>
</ul>
<p>The on-chip DMA indicated on the block diagram can transfer data between the Weight Memory, the L2, and the L1.</p>
<p align="center">
 <img src="../_static/tutorials/introduction/Siracusa.png" alt="Description" width="75%">
</p>
<p>Now that you understand the hardware and the kind of workload we want to execute. Let’s deploy using various optimizations to study their impact. The first parameter we can play with is the number of cores from the RV32 cluster to use.</p>
<blockquote>
<div><p>✅ <strong>Task:</strong> Measure and compare the runtime of the <code class="docutils literal notranslate"><span class="pre">microLlama128</span></code> model using 1 and 8 cores. Compute the speedup ratio; why is it not 8?</p>
</div></blockquote>
<p><em>Hint:</em> <code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">testRunner_siracusa.py</span> <span class="pre">--help</span></code> will list and explain the available flags.</p>
<details>
 <summary><span style="font-weight: bold; font-size: 1.3em;">Solution</span></summary>
<blockquote>
<div><p>If you run <code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">testRunner_siracusa.py</span> <span class="pre">-t</span> <span class="pre">Tests/Models/microLlama/microLlama128</span> <span class="pre">--cores=1</span></code> and then <code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">testRunner_siracusa.py</span> <span class="pre">-t</span> <span class="pre">Tests/Models/microLlama/microLlama128</span> <span class="pre">--cores=8</span></code>, you should measure a runtime of ~16,1M cycles for 1 core and 3.1M cycles for 8 cores.</p>
<p>The speedup ratio is obtained via <span class="math notranslate nohighlight">\(\frac{\text{Runtime 1 cores}}{\text{Runtime 8 cores}} = 5.2\)</span>. Hence, using 8 cores instead of 1 leads to a 5.2 times speedup.</p>
<p>So why is the speedup ratio below 8? Mostly because all data movement is not overlapped with computation. Additionally, some kernels are probably not optimally parallelized for this specific network.</p>
</div></blockquote>
</details>
</section>
<section id="tiling-basics">
<h3>Tiling Basics<a class="headerlink" href="#tiling-basics" title="Link to this heading"></a></h3>
<p>It’s due time to talk about data movement now! We use all 8 cluster cores, which is great, but where do these cores fetch the data from? By default, when using <code class="docutils literal notranslate"><span class="pre">testRunner_siracusa.py</span></code>, all data is in L2; there is no tiling, and cores read and write data directly to/from L2. As the L2 memory is “further away” from the cluster, load/store takes several cycles, which is non-optimal.</p>
<p>What we really want is to use the L1 memory, which provides 1 cycle latency load/store! But as the capacity is relatively small (256KB), we need to <strong>tile our layers</strong>. Tiling operands for an accelerator featuring only scratchpad memories is not trivial (unlike in architectures with data caches). For each layer, the compiler has to decide on tile size, a tiling schedule, a buffering strategy (single buffer, double buffer, etc…), and a memory allocation strategy. Then, the compiler must generate the code to configure and launch each transfer and place barriers accordingly to maximize concurrency.</p>
<p>The good news is that Deeploy can already do that! So, let’s generate and run some tiled code to see the impact of tiling on the runtime.</p>
<blockquote>
<div><p>✅ <strong>Task:</strong> Get familiar with the CLI arguments of <code class="docutils literal notranslate"><span class="pre">testRunner_tiled_siracusa.py</span></code>, then run <code class="docutils literal notranslate"><span class="pre">microLlama64_parallel</span></code> with different configurations. Find one “bad” and one “good” configuration, and explain why.</p>
</div></blockquote>
<p><em>Hint:</em> Use the <code class="docutils literal notranslate"><span class="pre">--help</span></code> flag to list and explain the available flags.</p>
<details>
 <summary><span style="font-weight: bold; font-size: 1.3em;">Solution</span></summary>
<blockquote>
<div><p>Bad configuration: <code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">testRunner_tiled_siracusa.py</span> <span class="pre">-t</span> <span class="pre">Tests/Models/microLlama/microLlama64_parallel</span> <span class="pre">--cores=8</span> <span class="pre">--l1</span> <span class="pre">8000</span> <span class="pre">--defaultMemLevel=L2</span></code> -&gt; Runtime: 47.5 MCycles</p>
<p>Good configuration <code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">testRunner_tiled_siracusa.py</span> <span class="pre">-t</span> <span class="pre">Tests/Models/microLlama/microLlama64_parallel</span> <span class="pre">--cores=8</span> <span class="pre">--l1</span> <span class="pre">64000</span> <span class="pre">--defaultMemLevel=L2</span></code>: -&gt; Runtime: 35.3 MCycles</p>
<p>Justification: As the size of the L1 memory gets smaller, tiles also get smaller and smaller. Smaller tiles usually mean that it’s harder to keep the core properly utilized.</p>
</div></blockquote>
</details>
</section>
<section id="profiling-the-execution">
<h3>Profiling the Execution<a class="headerlink" href="#profiling-the-execution" title="Link to this heading"></a></h3>
<p>To measure the effect of some optimizations in more detail, you can use the <code class="docutils literal notranslate"><span class="pre">--profileTiling=L2</span></code> flag. This flag will enable a code transformation that will insert print displaying the runtime of several critical code sections. For instance, profiling an <em>Integer Layer Normalization</em> layer from L2 with two tiles will return the print the following:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">INTEGER_RMSNORM</span> <span class="n">L2</span><span class="p">][</span><span class="n">SB</span><span class="p">][</span><span class="mi">0</span> <span class="n">ops</span><span class="p">][</span><span class="n">Tile</span> <span class="mi">0</span><span class="p">]</span> <span class="n">Input</span> <span class="n">DMA</span> <span class="n">took</span> <span class="mi">489</span> <span class="n">cycles</span>
<span class="p">[</span><span class="n">INTEGER_RMSNORM</span> <span class="n">L2</span><span class="p">][</span><span class="n">SB</span><span class="p">][</span><span class="mi">0</span> <span class="n">ops</span><span class="p">][</span><span class="n">Tile</span> <span class="mi">0</span><span class="p">]</span> <span class="n">Kernel</span> <span class="n">took</span> <span class="mi">43305</span> <span class="n">cycles</span>
<span class="p">[</span><span class="n">INTEGER_RMSNORM</span> <span class="n">L2</span><span class="p">][</span><span class="n">SB</span><span class="p">][</span><span class="mi">0</span> <span class="n">ops</span><span class="p">][</span><span class="n">Tile</span> <span class="mi">0</span><span class="p">]</span> <span class="n">Output</span> <span class="n">DMA</span> <span class="n">took</span> <span class="mi">534</span> <span class="n">cycles</span>
<span class="p">[</span><span class="n">INTEGER_RMSNORM</span> <span class="n">L2</span><span class="p">][</span><span class="n">SB</span><span class="p">][</span><span class="mi">0</span> <span class="n">ops</span><span class="p">][</span><span class="n">Tile</span> <span class="mi">1</span><span class="p">]</span> <span class="n">Input</span> <span class="n">DMA</span> <span class="n">took</span> <span class="mi">82</span> <span class="n">cycles</span>
<span class="p">[</span><span class="n">INTEGER_RMSNORM</span> <span class="n">L2</span><span class="p">][</span><span class="n">SB</span><span class="p">][</span><span class="mi">0</span> <span class="n">ops</span><span class="p">][</span><span class="n">Tile</span> <span class="mi">1</span><span class="p">]</span> <span class="n">Kernel</span> <span class="n">took</span> <span class="mi">3254</span> <span class="n">cycles</span>
<span class="p">[</span><span class="n">INTEGER_RMSNORM</span> <span class="n">L2</span><span class="p">][</span><span class="n">SB</span><span class="p">][</span><span class="mi">0</span> <span class="n">ops</span><span class="p">][</span><span class="n">Tile</span> <span class="mi">1</span><span class="p">]</span> <span class="n">Output</span> <span class="n">DMA</span> <span class="n">took</span> <span class="mi">49</span> <span class="n">cycles</span>
</pre></div>
</div>
<p>With this profiling trace, you can clearly measure the overhead of DMA transfers. When the profiling is turned ON, the total runtime of the application will encompass the prints.</p>
</section>
<section id="using-the-npu-and-the-neural-memory-subsystem-nms">
<h3>Using the NPU and the Neural Memory Subsystem (NMS)<a class="headerlink" href="#using-the-npu-and-the-neural-memory-subsystem-nms" title="Link to this heading"></a></h3>
<p>To use the NPU, you can use the <code class="docutils literal notranslate"><span class="pre">testRunner_tiled_siracusa_w_neureka.py</span></code>. The Linear layers will automatically be executed by the NPU. To enable the NMS, use the <code class="docutils literal notranslate"><span class="pre">--neureka-wmem</span></code> flag. When the NMS is enabled, the constant tensors used by the accelerator will be placed in the Weight Memory.</p>
<blockquote>
<div><p>✅ <strong>Task:</strong> Execute Micro Llama in parallel and autoregressive mode using the NPU, derive the speedup at the model level and at the layer level compared to execution without NPU.</p>
</div></blockquote>
<p><em>Hint:</em> Save the profiling traces somewhere to reason about them later on.</p>
<blockquote>
<div><p>✅ <strong>Task:</strong> Why does the NPU bring more speedup in parallel mode than in autoregressive mode?</p>
</div></blockquote>
<details>
 <summary><span style="font-weight: bold; font-size: 1.3em;">Solution</span></summary>
<blockquote>
<div><p>The runtime in parallel mode with NPU is obtained with:</p>
<p><code class="docutils literal notranslate"> <span class="pre">python</span> <span class="pre">testRunner_tiled_siracusa_w_neureka.py</span> <span class="pre">-t</span> <span class="pre">Tests/Models/microLlama/microLlama64_parallel</span> <span class="pre">--cores=8</span> <span class="pre">--l1</span> <span class="pre">64000</span> <span class="pre">--defaultMemLevel=L2</span> </code></p>
<p>And returns 28.6 MCycles of runtime. The runtime without NPU was measured above and is 35.3 MCycles. Hence, the speedup is ~1.23 times.</p>
<p>We apply the same methodology on <code class="docutils literal notranslate"><span class="pre">microLlama64</span></code> and get a speedup of ~1.04 times.</p>
<p>Now, why is the speedup lesser in autoregressive mode compared to parallel mode? This is because the parallel mode is composed mainly of GEMM, while the autoregressive mode uses GEMV. With GEMV, the accelerator is underutilized as the <a class="reference external" href="https://spcl.inf.ethz.ch/Teaching/2013-dphpc/lecture9-6up.pdf">operational intensity</a> of GEMV is very low, especially compared to GEMM.</p>
<p>Additionally, in autoregressive mode (unlike in parallel mode), you have to load the KV cache, which requires lots of data movement not accelerated by the NPU.</p>
</div></blockquote>
</details>
<br>
<blockquote>
<div><p>✅ <strong>Task:</strong> Benchmark the effect of the NMS on the model runtime and at the layer level. Do you notice any speedup? If yes, where does it come from?</p>
</div></blockquote>
<details>
 <summary><span style="font-weight: bold; font-size: 1.3em;">Solution</span></summary>
<blockquote>
<div><p>Using the NMS brings the runtime from 857 to 780 KCycles for the autoregressive mode and from 28.6 to 28.3 MCycles for the parallel mode. By inspecting the trace, you can notice that the NMS drastically reduces the time spent on input DMA transfers for the layers offloaded to the NPU.</p>
<p>This is the profiling trace for a layer without using the NMS:</p>
</div></blockquote>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">RequantizedPwConv_L2</span><span class="p">][</span><span class="n">SB</span><span class="p">][</span><span class="mi">32771</span> <span class="n">ops</span><span class="p">][</span><span class="n">Tile</span> <span class="mi">0</span><span class="p">]</span> <span class="n">Input</span> <span class="n">DMA</span> <span class="n">took</span> <span class="mi">2037</span> <span class="n">cycles</span>
<span class="p">[</span><span class="n">RequantizedPwConv_L2</span><span class="p">][</span><span class="n">SB</span><span class="p">][</span><span class="mi">32771</span> <span class="n">ops</span><span class="p">][</span><span class="n">Tile</span> <span class="mi">0</span><span class="p">]</span> <span class="n">Kernel</span> <span class="n">took</span> <span class="mi">2649</span> <span class="n">cycles</span>
<span class="p">[</span><span class="n">RequantizedPwConv_L2</span><span class="p">][</span><span class="n">SB</span><span class="p">][</span><span class="mi">32771</span> <span class="n">ops</span><span class="p">][</span><span class="n">Tile</span> <span class="mi">0</span><span class="p">]</span> <span class="n">Output</span> <span class="n">DMA</span> <span class="n">took</span> <span class="mi">50</span> <span class="n">cycles</span>
</pre></div>
</div>
<blockquote>
<div><p>And this is with the NMS activated:</p>
</div></blockquote>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">RequantizedPwConv_L2</span><span class="p">][</span><span class="n">SB</span><span class="p">][</span><span class="mi">32771</span> <span class="n">ops</span><span class="p">][</span><span class="n">Tile</span> <span class="mi">0</span><span class="p">]</span> <span class="n">Input</span> <span class="n">DMA</span> <span class="n">took</span> <span class="mi">125</span> <span class="n">cycles</span>
<span class="p">[</span><span class="n">RequantizedPwConv_L2</span><span class="p">][</span><span class="n">SB</span><span class="p">][</span><span class="mi">32771</span> <span class="n">ops</span><span class="p">][</span><span class="n">Tile</span> <span class="mi">0</span><span class="p">]</span> <span class="n">Kernel</span> <span class="n">took</span> <span class="mi">2595</span> <span class="n">cycles</span>
<span class="p">[</span><span class="n">RequantizedPwConv_L2</span><span class="p">][</span><span class="n">SB</span><span class="p">][</span><span class="mi">32771</span> <span class="n">ops</span><span class="p">][</span><span class="n">Tile</span> <span class="mi">0</span><span class="p">]</span> <span class="n">Output</span> <span class="n">DMA</span> <span class="n">took</span> <span class="mi">56</span> <span class="n">cycles</span>
</pre></div>
</div>
</details>
<br>
<blockquote>
<div><p>✅ <strong>Task:</strong> Why does the autoregressive mode benefit more from the NMS than the parallel mode?</p>
</div></blockquote>
<details>
 <summary><span style="font-weight: bold; font-size: 1.3em;">Solution</span></summary>
<blockquote>
<div><p>Using the NMS relaxes the memory boundness of the NPU. In the GEMM, we are not in a memory-bound regime, and the DMA transfer overhead is negligible with regard to the total runtime. In the autoregressive mode, we spend a lot of time on DMA transfers; hence, providing more bandwidth to the accelerator is very beneficial.</p>
</div></blockquote>
</details>
<br>
<p>Et voilà, this is the end of the tutorial. Thank you for following it until the end. If you are interested in learning more about Deeploy or the SoCs we develop at the <a class="reference external" href="https://pulp-platform.org/">PULP Platform</a>, please reach out!</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="overview.html" class="btn btn-neutral float-left" title="Tutorials" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="debugging.html" class="btn btn-neutral float-right" title="Debugging" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, Moritz Scherer, Philip Wiese, Luka Macan, Victor Jung.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <!--
SPDX-FileCopyrightText: 2025 ETH Zurich and University of Bologna

SPDX-License-Identifier: Apache-2.0
-->

<div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    Version: main
    <span class="fa fa-caret-down"></span>
  </span>
  <div class="rst-other-versions">
    
    
    <dl>
      <dt>Versions</dt>
      
      <dd><a href="https://pulp-platform.github.io/Deeploy/index.html">main</a></dd>
      
      <dd><a href="https://pulp-platform.github.io/Deeploy/branch/devel/index.html">devel</a></dd>
      
      <dd><a href="https://pulp-platform.github.io/Deeploy/tag/v0.2.0/index.html">v0.2.0</a></dd>
      
    </dl>
    
    <br>
    </dl>
  </div>
</div><script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>